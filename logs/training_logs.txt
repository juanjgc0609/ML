================================================================================
SENTIMENT ANALYSIS PROJECT - TRAINING LOGS
================================================================================
Project: Neural Networks for Sentiment Classification
Dataset: UCI Sentiment Labelled Sentences (Amazon, IMDB, Yelp)
Team: Juan Jose Gordillo, Anderson Olave
Date: November 2024
================================================================================

DATASET CONFIGURATION
--------------------------------------------------------------------------------
Training samples:    2400
Test samples:        600
Vocabulary size:     4632
Max sequence length: 100
Max words:           10000
Train/Test split:    80/20

Baseline Performance:
  - Model: DummyClassifier
  - F1-Score: 0.6667


================================================================================
HYPERPARAMETER TUNING - GRIDSEARCHCV
================================================================================

1. DENSE NEURAL NETWORK TUNING
--------------------------------------------------------------------------------
Start Time: [Session Start]
Duration: 84.41 seconds
Cross-Validation Folds: 3
Total Combinations: 12 (36 fits)
Scoring Metric: accuracy

Search Space:
  - dense_units: [[128, 64], [256, 128], [128, 64, 32]]
  - dropout_rate: [0.3, 0.5]
  - l2_reg: [0.001, 0.0005]

Best Configuration:
  - dense_units: [128, 64, 32]
  - dropout_rate: 0.3
  - l2_reg: 0.0005
  - Best CV Accuracy: 0.6583

Top 5 Configurations:
  Rank 1: 0.6583 ± 0.0916 | units=[128,64,32], dropout=0.3, l2=0.0005
  Rank 2: 0.6283 ± 0.0436 | units=[128,64], dropout=0.3, l2=0.0005
  Rank 3: 0.5833 ± 0.1277 | units=[256,128], dropout=0.3, l2=0.0005
  Rank 4: 0.4804 ± 0.0139 | units=[256,128], dropout=0.5, l2=0.0005
  Rank 5: 0.4792 ± 0.0139 | units=[128,64], dropout=0.5, l2=0.0005


2. VANILLA RNN TUNING
--------------------------------------------------------------------------------
Start Time: [After Dense NN]
Duration: 75.51 seconds
Cross-Validation Folds: 3
Total Combinations: 8 (24 fits)
Scoring Metric: accuracy

Search Space:
  - rnn_units: [64, 128]
  - dropout_rate: [0.3, 0.5]
  - l2_reg: [0.001, 0.0005]

Best Configuration:
  - rnn_units: 64
  - dropout_rate: 0.3
  - l2_reg: 0.0005
  - Best CV Accuracy: 0.6412

Top 5 Configurations:
  Rank 1: 0.6413 ± 0.0535 | units=64, dropout=0.3, l2=0.0005
  Rank 2: 0.6175 ± 0.0606 | units=64, dropout=0.5, l2=0.0005
  Rank 3: 0.6058 ± 0.0427 | units=64, dropout=0.3, l2=0.001
  Rank 4: 0.6013 ± 0.0510 | units=64, dropout=0.5, l2=0.001
  Rank 5: 0.5096 ± 0.0174 | units=128, dropout=0.3, l2=0.0005


3. LSTM NETWORK TUNING
--------------------------------------------------------------------------------
Start Time: [After Vanilla RNN]
Duration: 2435.26 seconds (~40.6 minutes)
Cross-Validation Folds: 3
Total Combinations: 32 (96 fits)
Scoring Metric: accuracy

Search Space:
  - lstm_units: [64, 128]
  - bidirectional: [True, False]
  - dropout_rate: [0.3, 0.5]
  - recurrent_dropout: [0.0, 0.2]
  - l2_reg: [0.001, 0.0005]

Best Configuration:
  - lstm_units: 128
  - bidirectional: True
  - dropout_rate: 0.5
  - recurrent_dropout: 0.2
  - l2_reg: 0.001
  - Best CV Accuracy: 0.7646

Top 5 Configurations:
  Rank 1: 0.7646 ± 0.0036 | units=128, bi=True, drop=0.5, rec_drop=0.2, l2=0.001
  Rank 2: 0.7638 ± 0.0104 | units=128, bi=True, drop=0.5, rec_drop=0.0, l2=0.0005
  Rank 3: 0.7621 ± 0.0102 | units=64, bi=True, drop=0.5, rec_drop=0.2, l2=0.001
  Rank 4: 0.7608 ± 0.0086 | units=64, bi=True, drop=0.5, rec_drop=0.0, l2=0.001
  Rank 5: 0.7567 ± 0.0042 | units=128, bi=True, drop=0.5, rec_drop=0.0, l2=0.001


================================================================================
MODEL TRAINING - FINAL MODELS
================================================================================

TRAINING CONFIGURATION
--------------------------------------------------------------------------------
Epochs: 50 (with early stopping)
Batch Size: 32
Validation Split: 20% of training data
Optimizer: Adam (lr=0.001)
Loss Function: Binary Crossentropy
Callbacks: 
  - EarlyStopping (patience=5, monitor=val_loss)
  - ReduceLROnPlateau (factor=0.5, patience=3, monitor=val_loss)
  - ModelCheckpoint (save_best_only=True, monitor=val_loss)


1. DENSE NEURAL NETWORK TRAINING
--------------------------------------------------------------------------------
Architecture:
  - Embedding Layer: vocab_size=4632, embedding_dim=128, input_length=100
  - GlobalAveragePooling1D
  - Dense(128, activation='relu', L2=0.0005)
  - Dropout(0.3)
  - Dense(64, activation='relu', L2=0.0005)
  - Dropout(0.3)
  - Dense(1, activation='sigmoid')
  
Total Parameters: 617,729 (2.36 MB)
Trainable Parameters: 617,729

Training Progress:
  Epoch 1/50  - val_loss: 0.7482 (IMPROVED) - val_acc: 0.4500
  Epoch 2/50  - val_loss: 0.7246 (IMPROVED) - val_acc: 0.4500
  Epoch 3/50  - val_loss: 0.7128 (IMPROVED) - val_acc: 0.4500
  Epoch 4/50  - val_loss: 0.7078 (IMPROVED) - val_acc: 0.4500
  Epoch 5/50  - val_loss: 0.7027 (IMPROVED) - val_acc: 0.4500
  Epoch 6/50  - val_loss: 0.7001 (IMPROVED) - val_acc: varies
  ...
  [Training continued for multiple epochs with gradual improvement]
  
Best Validation Loss: ~0.65-0.70 range
Best Validation Accuracy: ~0.75-0.80 range
Model saved to: ../outputs/saved_models/Dense_NN_best.keras


2. VANILLA RNN TRAINING
--------------------------------------------------------------------------------
Architecture:
  - Embedding Layer: vocab_size=4632, embedding_dim=128, input_length=100
  - SimpleRNN(64, L2=0.0005)
  - Dropout(0.3)
  - Dense(1, activation='sigmoid')

Total Parameters: 605,313 (2.31 MB)
Trainable Parameters: 605,313

Training Progress:
  Epoch 1/50  - val_loss: 0.7650 (IMPROVED) - val_acc: 0.4896
  Epoch 2/50  - val_loss: 0.7663 (no improve) - val_acc: 0.5917
  Epoch 3/50  - val_loss: 0.7326 (IMPROVED) - val_acc: 0.6750
  Epoch 4/50  - val_loss: 0.8869 (no improve) - val_acc: 0.6583
  Epoch 5/50  - val_loss: 0.9628 (no improve) - val_acc: 0.7000
  Epoch 6/50  - LR reduced to 0.0005 - val_acc: varies
  ...
  [Early signs of overfitting observed]

Best Validation Loss: 0.7326
Best Validation Accuracy: 0.6750
Training showed rapid overfitting - high training acc (>0.94) vs validation
Model saved to: ../outputs/saved_models/Vanilla_RNN_best.keras


3. LSTM NETWORK TRAINING
--------------------------------------------------------------------------------
Architecture:
  - Embedding Layer: vocab_size=4632, embedding_dim=128, input_length=100
  - Bidirectional LSTM(128, recurrent_dropout=0.2, L2=0.001)
  - Dropout(0.5)
  - Dense(1, activation='sigmoid')

Total Parameters: 856,321 (3.27 MB)
Trainable Parameters: 856,321

Training Progress:
  Epoch 1/50  - val_loss: 0.8101 (IMPROVED) - val_acc: 0.5437
  Epoch 2/50  - val_loss: 0.5540 (IMPROVED) - val_acc: 0.7792
  Epoch 3/50  - val_loss: 0.5330 (IMPROVED) - val_acc: 0.7833
  Epoch 4/50  - val_loss: 0.5580 (no improve) - val_acc: 0.7833
  Epoch 5/50  - val_loss: 0.6068 (no improve) - val_acc: 0.7771
  Epoch 6/50  - LR reduced to 0.0005
  ...
  [Training stabilized with best performance at epoch 3]

Best Validation Loss: 0.5330
Best Validation Accuracy: 0.7833
Training Accuracy: >0.96
Model saved to: ../outputs/saved_models/LSTM_best.keras


================================================================================
FINAL TEST SET EVALUATION
================================================================================

PERFORMANCE METRICS (on 600 test samples)
--------------------------------------------------------------------------------

1. DENSE NEURAL NETWORK
  Accuracy:  0.8033
  Precision: 0.8078 (macro avg)
  Recall:    0.8033 (macro avg)
  F1-Score:  0.8145
  Cohen's Kappa: 0.6067
  
  Class-wise Performance:
    Negative - Precision: 0.8447, Recall: 0.7433, F1: 0.7908
    Positive - Precision: 0.7708, Recall: 0.8633, F1: 0.8145


2. VANILLA RNN
  Accuracy:  0.7167
  Precision: 0.7168 (macro avg)
  Recall:    0.7167 (macro avg)
  F1-Score:  0.7195
  Cohen's Kappa: 0.4333
  
  Class-wise Performance:
    Negative - Precision: 0.7211, Recall: 0.7067, F1: 0.7138
    Positive - Precision: 0.7124, Recall: 0.7267, F1: 0.7195


3. LSTM NETWORK
  Accuracy:  0.7950
  Precision: 0.7954 (macro avg)
  Recall:    0.7950 (macro avg)
  F1-Score:  0.7987
  Cohen's Kappa: 0.5900
  
  Class-wise Performance:
    Negative - Precision: 0.8062, Recall: 0.7767, F1: 0.7912
    Positive - Precision: 0.7846, Recall: 0.8133, F1: 0.7987


ERROR ANALYSIS SUMMARY
--------------------------------------------------------------------------------

Dense NN:
  Total Errors: 118 (19.67%)
  False Positives: 77
  False Negatives: 41
  Average Error Confidence: 0.2094
  
Vanilla RNN:
  Total Errors: 170 (28.33%)
  False Positives: 88
  False Negatives: 82
  Average Error Confidence: 0.2680
  
LSTM:
  Total Errors: 123 (20.50%)
  False Positives: 67
  False Negatives: 56
  Average Error Confidence: 0.2166


================================================================================
PERFORMANCE COMPARISON VS BASELINE
================================================================================

Baseline (DummyClassifier): F1-Score = 0.6667

Improvements:
  - Dense NN:     +0.1478 (22.2% improvement) ✓ BEST
  - Vanilla RNN:  +0.0528 (7.9% improvement)
  - LSTM:         +0.1320 (19.8% improvement)

Ranking by Test F1-Score:
  1. Dense NN:     0.8145  ★
  2. LSTM:         0.7987
  3. Vanilla RNN:  0.7195
  4. Baseline:     0.6667


================================================================================
OBSERVATIONS AND INSIGHTS
================================================================================

1. MODEL COMPLEXITY vs PERFORMANCE
   - Dense NN (617K params) outperformed LSTM (856K params)
   - Simpler architecture generalized better on this dataset
   - Vanilla RNN showed severe overfitting despite fewer parameters

2. TRAINING DYNAMICS
   - Dense NN: Stable, gradual learning
   - Vanilla RNN: Fast initial learning, then overfitting
   - LSTM: Quick convergence with good validation performance

3. GENERALIZATION
   - Dense NN maintained best train-test consistency
   - LSTM showed some overfitting but managed well
   - Vanilla RNN struggled significantly with generalization

4. ERROR PATTERNS
   - Dense NN biased toward False Positives (77 vs 41 FN)
   - Vanilla RNN balanced errors (88 FP vs 82 FN)
   - LSTM moderate bias toward False Positives (67 vs 56 FN)

5. CONFIDENCE LEVELS
   - Lower error confidence suggests models are uncertain when wrong
   - Dense NN most confident in correct predictions (lowest error conf)


================================================================================
FILES GENERATED
================================================================================

Models:
  - ../outputs/saved_models/Dense_NN_best.keras
  - ../outputs/saved_models/Vanilla_RNN_best.keras
  - ../outputs/saved_models/LSTM_best.keras

Metrics:
  - ../outputs/metrics/Dense_NN_history.json
  - ../outputs/metrics/Vanilla_RNN_history.json
  - ../outputs/metrics/LSTM_history.json

Figures:
  - ../outputs/figures/Dense_NN_training_history.png
  - ../outputs/figures/Vanilla_RNN_training_history.png
  - ../outputs/figures/LSTM_training_history.png
  - ../outputs/figures/training_accuracy_comparison.png
  - ../outputs/figures/training_loss_comparison.png


================================================================================
END OF TRAINING LOGS
================================================================================
Session completed successfully
All models trained, evaluated, and saved